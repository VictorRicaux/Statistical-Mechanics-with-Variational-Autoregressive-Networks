{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # p et q sont deux listes de vecteru de taille n représentant les probas conditionelles\n",
    "#     # ATTENTION en sortie du réseau de neurones, on a un vecteur de taille n\n",
    "#     # en effet, on  a besoin des probas conditionelles pour chaque i. Donc, on ne peut pas demander au réseau de \n",
    "#     # sortir direct la proba q_theta(s)\n",
    "#     # Donc, c'est dans la fonction loss qu'on va convertir ces probas conditionelles en proba d'observer s (par une simple multiplication)\n",
    "#     probasp=[]\n",
    "#     probasq=[]\n",
    "#     for pi in p:\n",
    "#         res=1\n",
    "#         for i in pi:\n",
    "#             res*=i\n",
    "#         probasp.append(res)\n",
    "#     for qi in q:\n",
    "#         res=1\n",
    "#         for i in qi:\n",
    "#             res*=i\n",
    "#         probasq.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kulback_Leibler(q,p): # p et q sont des listes de probas d'observation des mêmes s\n",
    "    for i in range(len(p)):\n",
    "        if p[i] == 0:\n",
    "            p[i] = 1e-10\n",
    "    result = 0 \n",
    "    for i in range(len(p)):\n",
    "        result += q[i]*np.log(q[i]/p[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([0.1,0.2,0.3,0.4])\n",
    "b=torch.tensor([0.25,0.25,0.25,0.25])\n",
    "Kulback_Leibler(b,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(models, q , p_0,   n_iter=100, lr=1e-2):\n",
    "    nombre_de_neurones=len(models)\n",
    "    optimizers=[torch.optim.Adam(models[i].parameters(), lr=lr) for i in range(nombre_de_neurones)]\n",
    "    p=p_0\n",
    "    for epoch in range(n_iter):\n",
    "        p='...'# à compléter\n",
    "        for i in range(nombre_de_neurones):\n",
    "            optimizers[i].zero_grad()\n",
    "            loss=Kulback_Leibler(p, q)\n",
    "            loss.backward()\n",
    "            optimizers[i]\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, n_iter=100, lr=1e-2):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    axs = [plt.subplot(2, 5, i) for i in range(1, 11)]\n",
    "    losses = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # What is Adam? Which other simpler optimizers can you use?\n",
    "\n",
    "    for epoch in range(n_iter):\n",
    "        optimizer.zero_grad() # What is this step? IMPORTANT LINE\n",
    "        loss = loss_fn(model(x_train), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "       \n",
    "        if epoch % (n_iter/10) == 0:\n",
    "            print(f'Epoch {epoch}: {loss.item()}')\n",
    "\n",
    "            # plot progress\n",
    "            ax_index = int(epoch // (n_iter/10))\n",
    "            plt.sca(axs[ax_index])\n",
    "            plt.plot(x_grid, target(x_grid), label='target')\n",
    "            plt.plot(x_grid, grab(model(x_grid)).squeeze(), label='model init')\n",
    "            \n",
    "    return losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "M = torch.zeros((n, n), dtype=torch.int)  # Crée une matrice de zéros de taille n x n\n",
    "        \n",
    "for i in range(n):\n",
    "    for j in range(i, n):\n",
    "        M[i][j] = 0\n",
    "\n",
    "# Remplit la partie inférieure de la diagonale avec des uns\n",
    "for i in range(1, n):\n",
    "        for j in range(i):\n",
    "                M[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.tensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, mask, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        return nn.functional.linear(input, self.weight * self.mask, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.8808])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAN(nn.Module):\n",
    "    def __init__(self, input_size, activation=torch.sigmoid):\n",
    "        super(VAN, self).__init__() #initialisation obligatoire\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Création de la matrice de masque : que des 0 sur et au dessus de la diagonale et que des 1 dessous\n",
    "        M = torch.zeros((input_size, input_size), dtype=torch.int)\n",
    "        for i in range(input_size):\n",
    "            for j in range(i, input_size):\n",
    "                M[i][j] = 0\n",
    "        for i in range(1, input_size):\n",
    "            for j in range(i):\n",
    "                M[i][j] = 1\n",
    "\n",
    "        self.fc1 = MaskedLinear(input_size, input_size, mask=M)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        # à cette ligne on a multiplié x par la matrice de masque (triangulaire inférieure), puis appliqué la fonction d'activation\n",
    "        # donc la première coordonnée de x vaut activation(0) (normal, s^_1 ne dépend de personne)\n",
    "        # il faut donc ajouter 0.5 à la première coordonnée pour montrer qu'elle vaut 0 et 1 avec proba 0.5\n",
    "        x[0] = 0.5\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "train_set=torch.zeros((5, 2))\n",
    "# 5 points de taille 2\n",
    "print(train_set)\n",
    "train_set[0][0]=torch.bernoulli(torch.tensor(0.5))\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, p_obj,  n_iter=100, lr=1e-2, train_size=100):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # p_obj est la distribution à approximer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    # il faut tirer un train_set grâce au modèle pour s'entraîner dessus\n",
    "    train_set=torch.zeros((train_size, model.input_size))\n",
    "    for i in range(train_size):\n",
    "        # pour tirer un x dans x_train, on tire une bernoulli de paramètre 0.5 : c'est s_1\n",
    "        # comment avoir s_2 ? ON fait passer le vecteur [s_1, 0, 0, 0] dans le réseau de neurones\n",
    "        # en sortie du réseau de neurones, on a y =[ s1 randomn, p(s2|s1), p(s3|s1), p(s4|s1)...]\n",
    "        # la deuxième coordonnée de y est p(s2|s1), donc on tire une bernoulli de paramètre p(s2|s1)\n",
    "        # puis on recommence !\n",
    "        # on fait ça train_size fois\n",
    "        train_set[i][0]=torch.bernoulli(torch.tensor(0.5))\n",
    "        for j in range(1, model.input_size):\n",
    "            y_pred=model(train_set[i])\n",
    "            p_j= y_pred[j] # c'est p(s_j|s_{i<j})\n",
    "            train_set[i][j]=torch.bernoulli(torch.tensor(p_j)) # on tire une bernoulli de paramètre p(s_j|s_{i<j}) pour la j-ème variable\n",
    "    \n",
    "    y_train=torch.tensor([p_obj(s) for s in train_set])\n",
    "    # à cette étape, on a un train set, on peut entraîner le modèle\n",
    "    for epoch in range(n_iter):\n",
    "        optimizer.zero_grad() # What is this step? IMPORTANT LINE\n",
    "        listes_de_probas_conditionelles=model(train_set) # on récupère les probas conditionelles, il faut les multiplier pour avoir les probas tout court\n",
    "        q_theta_predit=[]\n",
    "        for proba_conditionelle in listes_de_probas_conditionelles:\n",
    "            res=1\n",
    "            for i in proba_conditionelle:\n",
    "                res*=i\n",
    "            q_theta_predit.append(res)\n",
    "        # c'est bon on a les probas, on peut appliquer DKL\n",
    "        loss = Kulback_Leibler(torch.tensor(q_theta_predit), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if epoch % (n_iter/10) == 0:\n",
    "            print(f'Epoch {epoch}: {loss.item()}')\n",
    "    return losses\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6225, 0.8808])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([0.5, 2.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=7\n",
    "M = torch.zeros((n, n), dtype=torch.int)  # Crée une matrice de zéros de taille n x n\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i, n):\n",
    "        M[i][j] = 0\n",
    "\n",
    "# Remplit la partie inférieure de la diagonale avec des uns\n",
    "for i in range(1, n):\n",
    "        for j in range(i):\n",
    "                M[i][j] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    a MLP class inheriting from the parent class nn.Module. \n",
    "    nn.Module is the generic parent class of models in Pytorch.\n",
    "    It requires a method called forward. \n",
    "    \n",
    "    Pytorch will be able to recursively recover all parameters of the\n",
    "    attributes of a nn.Module object provided the attributes have themselves\n",
    "    type nn.Modules or nn.ModuleList.\n",
    "\n",
    "    In this implementation we explicitly specify the scale of initialization \n",
    "    of the weight matrices.\n",
    "    \"\"\"\n",
    "    def __init__(self, layerdims, activation=torch.relu, out_activation=None, init_scale=1):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layerdims = layerdims\n",
    "        self.activation = activation\n",
    "        self.out_activation = out_activation\n",
    "\n",
    "        linears = [\n",
    "            nn.Linear(layerdims[i], layerdims[i + 1]) for i in range(len(layerdims) - 1)\n",
    "        ]\n",
    "\n",
    "        if init_scale is not None:\n",
    "            for l, layer in enumerate(linears):\n",
    "                torch.nn.init.normal_(\n",
    "                    layer.weight, std=init_scale / np.sqrt(layerdims[l])\n",
    "                )\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "        self.linears = nn.ModuleList(linears)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = list(enumerate(self.linears))\n",
    "        for _, l in layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "        y = layers[-1][1](x)\n",
    "        if self.out_activation is not None:\n",
    "            y = self.out_activation(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in torch.tensor([1, 2, 3, 4, 5])==torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# in float\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(torch.tensor([1, 2, 3, 4, 5]), torch.tensor([1, 2, 3, 4, 5])).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p= torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "q= torch.tensor([0.25, 0.25, 0.25, 0.25])\n",
    "Kulback_Leibler(p, p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 497.21978759765625\n",
      "Epoch 10: 519.7166137695312\n",
      "Epoch 20: inf\n",
      "Epoch 30: 520.9984741210938\n",
      "Epoch 40: 520.3217163085938\n",
      "Epoch 50: inf\n",
      "Epoch 60: inf\n",
      "Epoch 70: inf\n",
      "Epoch 80: 497.7961120605469\n",
      "Epoch 90: 473.74334716796875\n"
     ]
    }
   ],
   "source": [
    "from VAN import *\n",
    "import matplotlib.pyplot as plt\n",
    "taille=6\n",
    "def p(s):\n",
    "    if torch.eq(s, torch.tensor([1, 1, 1, 0, 0, 0])).all().item():\n",
    "        return 0.5\n",
    "    elif torch.eq(s, torch.tensor([0, 0, 0, 1, 1, 1])).all().item():\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def p2(s):\n",
    "    return 1/(2**6)\n",
    "\n",
    "model=VAN(taille, torch.sigmoid)\n",
    "\n",
    "l=train(model, p, n_iter=100, lr=1e-1, train_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6969754099845886,\n",
       " -0.6694832444190979,\n",
       " -0.678429365158081,\n",
       " -0.7220017313957214,\n",
       " -0.6894797682762146,\n",
       " -0.6848087310791016,\n",
       " -0.6867111325263977,\n",
       " -0.6920499205589294,\n",
       " -0.6663811206817627,\n",
       " -0.658984899520874]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(torch.tensor(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.bernoulli(torch.tensor(0.6).detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.tensor(float(np.random.binomial(1, 0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 2.0000, 3.0000, 4.0000, 5.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1, 2, 3, 4, 5]).float()\n",
    "modified_x = x.clone()  # Create a copy of x to avoid modifying the original tensor\n",
    "modified_x[0] = 0.5 \n",
    "modified_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.tensor([1, 2, 3, 4, 5]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2086746e160>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc70lEQVR4nO3df5DUdf3A8dfeKXdWx+ZpBwcehs6oXCQeoISSDZMiWKeolZGK049pcigRZpwgVIYKrkSbZiJhcESdzGTKH0kaYRagaZESlsFgJBOMHlFD3R04YN3t9w/ivp7cAovH7fvuHo+Z/WM/+/msr51tuiefX5vJ5XK5AABIWEmxBwAAOBzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPKOK/YAXaWtrS1ef/31qKioiEwmU+xxAIAjkMvloqWlJQYNGhQlJfn3o/SaYHn99dejpqam2GMAAEdh+/btccopp+R9vdcES0VFRUTs/8D9+/cv8jQAwJFobm6Ompqa9r/j+fSaYDlwGKh///6CBQB6mMOdzuGkWwAgeYIFAEieYAEAkidYAIDkCRYAIHmCBQBInmABAJInWACA5PWaG8cB3a+1LRfrtu6KnS17o6qiPM4bWhmlJX7LC+h6ggU4Kitfbox5KzZGY9Pe9mXV2fKYW18bE4dXF3EyoDdySAgo2MqXG+OGB9Z3iJWIiB1Ne+OGB9bHypcbizQZ0FsJFqAgrW25mLdiY+Q6ee3AsnkrNkZrW2drABwdwQIUZN3WXQftWXmrXEQ0Nu2NdVt3dd9QQK/nHBagIDtb8sfK0awHb+dkbjojWICCVFWUd+l68FZO5iYfh4SAgpw3tDKqs+WR79+7mdj/B+a8oZXdORa9gJO5ORTBAhSktCQTc+trIyIOipYDz+fW19qFT0GczM3hCBagYBOHV8fia0fGwGzHwz4Ds+Wx+NqRdt1TMCdzczjOYQGOysTh1XFx7UAnR9IlnMzN4QgW4KiVlmRi7OknFXsMegEnc3M4ggWKwGWb0NGBk7l3NO3t9DyWTOw/5Ohk7r5LsEA3c9kmHOzAydw3PLA+MhEdosXJ3EQ46Ra6lcs2IT8nc3Mo9rBANzncZZuZ2H/Z5sW1A/0rkj7LydzkI1igmxRy2aYTWenLnMxNZxwSgm7isk2AoydYoJu4bBPg6AkW6CZ+gwfg6AkW6CZ+gwfg6AkW6EYu2wQ4Oq4Sgm7msk2AwgmWQ3D7dI4Vl20CFEaw5OH26QCQDuewdMLt0wEgLYLlbQ53+/SI/bdPb23rbA0A4FgQLG9TyO3TAYDuIVjexu3TASA9guVt3D4dANIjWN7G7dMBID2C5W3cPh0A0iNYOuH26QCQFjeOy8Pt0wEgHYLlENw+HQDS4JAQAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8vz4IQCQV2tbLtZt3RU7W/ZGVUV5nDe0MkpLMt0+h2ABADq18uXGmLdiYzQ27W1fVp0tj7n1tTFxeHW3zuKQEABwkJUvN8YND6zvECsRETua9sYND6yPlS83dus8ggUA6KC1LRfzVmyMXCevHVg2b8XGaG3rbI1jQ7AAAB2s27rroD0rb5WLiMamvbFu665um0mwAAAd7GzJHytHs15XECwAQAdVFeVdul5XECwAQAfnDa2M6mx55Lt4ORP7rxY6b2hlt80kWACADkpLMjG3vjYi4qBoOfB8bn1tt96PRbAAAAeZOLw6Fl87MgZmOx72GZgtj8XXjuz2+7C4cRwA0KmJw6vj4tqB7nQLAKSttCQTY08/qdhjOCQEAKRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8goOlrVr10Z9fX0MGjQoMplMPPbYYx1ez2QynT4WLlyY9z3vvvvu+PCHPxwnnnhinHjiiXHRRRfFunXrCv4wAEDvVHCw7NmzJ0aMGBGLFi3q9PXGxsYOj2XLlkUmk4mrrroq73uuXr06pkyZEr/+9a/j+eefjyFDhsSECRPitddeK3Q8AKAXyuRyudxRb5zJxKOPPhqTJ0/Ou87kyZOjpaUlnn766SN+39bW1jjxxBNj0aJFMXXq1CPaprm5ObLZbDQ1NUX//v2P+L8FABTPkf79Pqa35v/73/8eTzzxRNx///0FbffGG2/Ef/7zn6iszP+z1fv27Yt9+/a1P29ubj7qOQGAtB3Tk27vv//+qKioiCuvvLKg7WbNmhWDBw+Oiy66KO86DQ0Nkc1m2x81NTXvdFwAIFHHNFiWLVsW11xzTZSXlx9+5f+5/fbb40c/+lE88sgjh9xu9uzZ0dTU1P7Yvn17V4wMACTomB0SeuaZZ2Lz5s2xfPnyI97mjjvuiAULFsQvf/nLOPvssw+5bllZWZSVlb3TMQGAHuCYBcs999wTo0aNihEjRhzR+gsXLoxvfvOb8Ytf/CJGjx59rMYCAHqggoNl9+7dsWXLlvbnW7dujQ0bNkRlZWUMGTIkIvafAPvjH/847rzzzk7fY+rUqTF48OBoaGiIiP2HgW699dZ48MEH4/3vf3/s2LEjIiLe8573xHve856CPxQA0LsUfA7LCy+8EHV1dVFXVxcRETNnzoy6urq47bbb2td56KGHIpfLxZQpUzp9j23btkVjY2P787vuuivefPPN+MQnPhHV1dXtjzvuuKPQ8QCAXugd3YclJe7DAgA9z5H+/fZbQgBA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkLyCg2Xt2rVRX18fgwYNikwmE4899liH1zOZTKePhQsXHvJ9H3744aitrY2ysrKora2NRx99tNDRAIBequBg2bNnT4wYMSIWLVrU6euNjY0dHsuWLYtMJhNXXXVV3vd8/vnn4+qrr47rrrsuXnrppbjuuuviU5/6VPzud78rdDwAoBfK5HK53FFvnMnEo48+GpMnT867zuTJk6OlpSWefvrpvOtcffXV0dzcHD//+c/bl02cODFOPPHE+NGPfnREszQ3N0c2m42mpqbo37//EX8GAKB4jvTv9zE9h+Xvf/97PPHEE/H5z3/+kOs9//zzMWHChA7LLrnkknjuuefybrNv375obm7u8AAAeqdjGiz3339/VFRUxJVXXnnI9Xbs2BEDBgzosGzAgAGxY8eOvNs0NDRENpttf9TU1HTJzABAeo5psCxbtiyuueaaKC8vP+y6mUymw/NcLnfQsreaPXt2NDU1tT+2b9/+jucFANJ03LF642eeeSY2b94cy5cvP+y6AwcOPGhvys6dOw/a6/JWZWVlUVZW9o7nBADSd8z2sNxzzz0xatSoGDFixGHXHTt2bDz11FMdlq1atSrOP//8YzUeANCDFLyHZffu3bFly5b251u3bo0NGzZEZWVlDBkyJCL2n/H74x//OO68885O32Pq1KkxePDgaGhoiIiI6dOnx4UXXhjf/va34/LLL4+f/vSn8ctf/jKeffbZo/lMAEAvU/AelhdeeCHq6uqirq4uIiJmzpwZdXV1cdttt7Wv89BDD0Uul4spU6Z0+h7btm2LxsbG9ufnn39+PPTQQ3HvvffG2WefHffdd18sX748xowZU+h4AEAv9I7uw5IS92EBgJ4nifuwAAB0BcECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJKzhY1q5dG/X19TFo0KDIZDLx2GOPHbTOpk2b4rLLLotsNhsVFRXxoQ99KLZt23bI9/3ud78bZ555ZpxwwglRU1MTM2bMiL179xY6HgDQCxUcLHv27IkRI0bEokWLOn39r3/9a4wbNy7OOuusWL16dbz00ktx6623Rnl5ed73/OEPfxizZs2KuXPnxqZNm+Kee+6J5cuXx+zZswsdDwDohY4rdINJkybFpEmT8r4+Z86cuPTSS+P2229vX3baaacd8j2ff/75uOCCC+Izn/lMRES8//3vjylTpsS6desKHQ8A6IW69ByWtra2eOKJJ+KMM86ISy65JKqqqmLMmDGdHjZ6q3HjxsWLL77YHiivvvpqPPnkk/Gxj30s7zb79u2L5ubmDg8AoHfq0mDZuXNn7N69O771rW/FxIkTY9WqVXHFFVfElVdeGWvWrMm73ac//en4xje+EePGjYvjjz8+Tj/99Bg/fnzMmjUr7zYNDQ2RzWbbHzU1NV35UQCAhHT5HpaIiMsvvzxmzJgR55xzTsyaNSs+/vGPx5IlS/Jut3r16pg/f37cddddsX79+njkkUfiZz/7WXzjG9/Iu83s2bOjqamp/bF9+/au/CgAQEIKPoflUE4++eQ47rjjora2tsPyYcOGxbPPPpt3u1tvvTWuu+66+MIXvhARER/84Adjz5498cUvfjHmzJkTJSUHd1VZWVmUlZV15fgAQKK6dA9Lv3794txzz43Nmzd3WP7KK6/Eqaeemne7N95446AoKS0tjVwuF7lcritHBAB6oIL3sOzevTu2bNnS/nzr1q2xYcOGqKysjCFDhsTNN98cV199dVx44YUxfvz4WLlyZaxYsSJWr17dvs3UqVNj8ODB0dDQEBER9fX18Z3vfCfq6upizJgxsWXLlrj11lvjsssui9LS0nf+KQGAHq3gYHnhhRdi/Pjx7c9nzpwZERHXX3993HfffXHFFVfEkiVLoqGhIW688cY488wz4+GHH45x48a1b7Nt27YOe1RuueWWyGQyccstt8Rrr70W73vf+6K+vj7mz5//Tj4bANBLZHK95JhLc3NzZLPZaGpqiv79+xd7HADgCBzp32+/JQQAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkrOFjWrl0b9fX1MWjQoMhkMvHYY48dtM6mTZvisssui2w2GxUVFfGhD30otm3bdsj3/fe//x3Tpk2L6urqKC8vj2HDhsWTTz5Z6HgAQC90XKEb7NmzJ0aMGBGf/exn46qrrjro9b/+9a8xbty4+PznPx/z5s2LbDYbmzZtivLy8rzv+eabb8bFF18cVVVV8ZOf/CROOeWU2L59e1RUVBQ6HgDQCxUcLJMmTYpJkyblfX3OnDlx6aWXxu23396+7LTTTjvkey5btix27doVzz33XBx//PEREXHqqacWOhoA0Et16TksbW1t8cQTT8QZZ5wRl1xySVRVVcWYMWM6PWz0Vo8//niMHTs2pk2bFgMGDIjhw4fHggULorW1tSvHAwB6qC4Nlp07d8bu3bvjW9/6VkycODFWrVoVV1xxRVx55ZWxZs2avNu9+uqr8ZOf/CRaW1vjySefjFtuuSXuvPPOmD9/ft5t9u3bF83NzR0eAEDvVPAhoUNpa2uLiIjLL788ZsyYERER55xzTjz33HOxZMmS+MhHPpJ3u6qqqli6dGmUlpbGqFGj4vXXX4+FCxfGbbfd1uk2DQ0NMW/evK4cHwBIVJfuYTn55JPjuOOOi9ra2g7Lhw0bdsirhKqrq+OMM86I0tLSDtvs2LEj3nzzzU63mT17djQ1NbU/tm/f3jUfAgBITpcGS79+/eLcc8+NzZs3d1j+yiuvHPIk2gsuuCC2bNnSvofmwDbV1dXRr1+/TrcpKyuL/v37d3gAAL1TwYeEdu/eHVu2bGl/vnXr1tiwYUNUVlbGkCFD4uabb46rr746Lrzwwhg/fnysXLkyVqxYEatXr27fZurUqTF48OBoaGiIiIgbbrghvve978X06dPjK1/5SvzlL3+JBQsWxI033vjOPyHAYbS25WLd1l2xs2VvVFWUx3lDK6O0JFPssYC3KDhYXnjhhRg/fnz785kzZ0ZExPXXXx/33XdfXHHFFbFkyZJoaGiIG2+8Mc4888x4+OGHY9y4ce3bbNu2LUpK/n/nTk1NTaxatSpmzJgRZ599dgwePDimT58eX/3qV9/JZwM4rJUvN8a8FRujsWlv+7LqbHnMra+NicOrizgZ8FaZXC6XK/YQXaG5uTmy2Ww0NTU5PAQckZUvN8YND6yPt/+f4IF9K4uvHSla4Bg70r/ffksI6JNa23Ixb8XGg2IlItqXzVuxMVrbesW/6aDHEyxAn7Ru664Oh4HeLhcRjU17Y93WXd03FJCXYAH6pJ0t+WPlaNYDji3BAvRJVRX5f5D1aNYDji3BAvRJ5w2tjOpseeS7eDkT+68WOm9oZXeOBeQhWIA+qbQkE3Pr99+V++3RcuD53Ppa92OBRAgWoM+aOLw6Fl87MgZmOx72GZgtd0kzJKZLf/wQoKeZOLw6Lq4d6E63kDjBAvR5pSWZGHv6ScUeAzgEh4QAgOQJFgAgeYIFAEieYAEAkidYAIDkCRYAIHkua6bHaG3LuVcGQB8lWOgRVr7cGPNWbIzGpv//5dzqbHnMra91N1KAPsAhIZK38uXGuOGB9R1iJSJiR9PeuOGB9bHy5cYiTQZAdxEsJK21LRfzVmyMXCevHVg2b8XGaG3rbA0AegvBQtLWbd110J6Vt8pFRGPT3li3dVf3DQVAtxMsJG1nS/5YOZr1AOiZBAtJq6oo79L1AOiZBAtJO29oZVRnyyPfxcuZ2H+10HlDK7tzLAC6mWAhaaUlmZhbXxsRcVC0HHg+t77W/VgAejnBQvImDq+OxdeOjIHZjod9BmbLY/G1I92HBaAPcOM4eoSJw6vj4tqB7nQL0EcJFnqM0pJMjD39pGKPAUAROCQEACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMkTLABA8gQLAJA8wQIAJK/X3Ok2l8tFRERzc3ORJwEAjtSBv9sH/o7n02uCpaWlJSIiampqijwJAFColpaWyGazeV/P5A6XND1EW1tbvP7661FRURGZTNf9IF5zc3PU1NTE9u3bo3///l32vhwd30d6fCdp8X2kxfdxeLlcLlpaWmLQoEFRUpL/TJVes4elpKQkTjnllGP2/v379/c/toT4PtLjO0mL7yMtvo9DO9SelQOcdAsAJE+wAADJEyyHUVZWFnPnzo2ysrJij0L4PlLkO0mL7yMtvo+u02tOugUAei97WACA5AkWACB5ggUASJ5gAQCSJ1gO46677oqhQ4dGeXl5jBo1Kp555plij9QnNTQ0xLnnnhsVFRVRVVUVkydPjs2bNxd7LP6noaEhMplM3HTTTcUepc967bXX4tprr42TTjop3vWud8U555wTL774YrHH6rP++9//xi233BJDhw6NE044IU477bT4+te/Hm1tbcUerccSLIewfPnyuOmmm2LOnDnxhz/8IT784Q/HpEmTYtu2bcUerc9Zs2ZNTJs2LX7729/GU089Ff/9739jwoQJsWfPnmKP1uf9/ve/j6VLl8bZZ59d7FH6rH/9619xwQUXxPHHHx8///nPY+PGjXHnnXfGe9/73mKP1md9+9vfjiVLlsSiRYti06ZNcfvtt8fChQvje9/7XrFH67Fc1nwIY8aMiZEjR8bixYvblw0bNiwmT54cDQ0NRZyMf/zjH1FVVRVr1qyJCy+8sNjj9Fm7d++OkSNHxl133RXf/OY345xzzonvfve7xR6rz5k1a1b85je/sQc4IR//+MdjwIABcc8997Qvu+qqq+Jd73pX/OAHPyjiZD2XPSx5vPnmm/Hiiy/GhAkTOiyfMGFCPPfcc0WaigOampoiIqKysrLIk/Rt06ZNi4997GNx0UUXFXuUPu3xxx+P0aNHxyc/+cmoqqqKurq6uPvuu4s9Vp82bty4ePrpp+OVV16JiIiXXnopnn322bj00kuLPFnP1Wt+/LCr/fOf/4zW1tYYMGBAh+UDBgyIHTt2FGkqIvb/sufMmTNj3LhxMXz48GKP02c99NBDsX79+vj9739f7FH6vFdffTUWL14cM2fOjK997Wuxbt26uPHGG6OsrCymTp1a7PH6pK9+9avR1NQUZ511VpSWlkZra2vMnz8/pkyZUuzReizBchiZTKbD81wud9AyuteXv/zl+OMf/xjPPvtssUfps7Zv3x7Tp0+PVatWRXl5ebHH6fPa2tpi9OjRsWDBgoiIqKuriz//+c+xePFiwVIky5cvjwceeCAefPDB+MAHPhAbNmyIm266KQYNGhTXX399scfrkQRLHieffHKUlpYetDdl586dB+11oft85StficcffzzWrl0bp5xySrHH6bNefPHF2LlzZ4waNap9WWtra6xduzYWLVoU+/bti9LS0iJO2LdUV1dHbW1th2XDhg2Lhx9+uEgTcfPNN8esWbPi05/+dEREfPCDH4y//e1v0dDQIFiOknNY8ujXr1+MGjUqnnrqqQ7Ln3rqqTj//POLNFXflcvl4stf/nI88sgj8atf/SqGDh1a7JH6tI9+9KPxpz/9KTZs2ND+GD16dFxzzTWxYcMGsdLNLrjggoMu83/llVfi1FNPLdJEvPHGG1FS0vFPbGlpqcua3wF7WA5h5syZcd1118Xo0aNj7NixsXTp0ti2bVt86UtfKvZofc60adPiwQcfjJ/+9KdRUVHRvucrm83GCSecUOTp+p6KioqDzh9697vfHSeddJLziopgxowZcf7558eCBQviU5/6VKxbty6WLl0aS5cuLfZofVZ9fX3Mnz8/hgwZEh/4wAfiD3/4Q3znO9+Jz33uc8UerefKcUjf//73c6eeemquX79+uZEjR+bWrFlT7JH6pIjo9HHvvfcWezT+5yMf+Uhu+vTpxR6jz1qxYkVu+PDhubKystxZZ52VW7p0abFH6tOam5tz06dPzw0ZMiRXXl6eO+2003Jz5szJ7du3r9ij9VjuwwIAJM85LABA8gQLAJA8wQIAJE+wAADJEywAQPIECwCQPMECACRPsAAAyRMsAEDyBAsAkDzBAgAkT7AAAMn7P5P8JvMZNc0XAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l, 'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "map588",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
